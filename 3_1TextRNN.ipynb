{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"i like dog\", \"i love coffee\", \"i hate milk\"]\n",
    "\n",
    "word_list = \" \".join(sentences).split()\n",
    "word_list = list(set(word_list))\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "number_dict = {i: w for i, w in enumerate(word_list)}\n",
    "n_class = len(word_dict)\n",
    "batch_size = len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'coffee': 0, 'dog': 1, 'like': 2, 'love': 3, 'milk': 4, 'hate': 5, 'i': 6}\n"
     ]
    }
   ],
   "source": [
    "print(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'like', 'dog']\n",
      "[6, 2]\n",
      "['i', 'love', 'coffee']\n",
      "[6, 3]\n",
      "['i', 'hate', 'milk']\n",
      "[6, 5]\n"
     ]
    }
   ],
   "source": [
    "input_batch, target_batch = [], []\n",
    "for sen in sentences:\n",
    "    word = sen.split()\n",
    "    print(word)\n",
    "    input1 = [word_dict[n] for n in word[:-1]]\n",
    "    target = word_dict[word[-1]]\n",
    "    print(input1)\n",
    "    \n",
    "    input_batch.append(np.eye(n_class)[input1])\n",
    "    target_batch.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = torch.FloatTensor(input_batch)\n",
    "target_batch = torch.LongTensor(target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 1., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 1., 0.]]])\n",
      "torch.Size([3, 2, 7])\n"
     ]
    }
   ],
   "source": [
    "print(input_batch)\n",
    "print(input_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 4])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 0., 1.]],\n",
      "\n",
      "        [[0., 0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 1., 0.]]])\n",
      "torch.Size([2, 3, 7])\n"
     ]
    }
   ],
   "source": [
    "input_batch1 = input_batch.transpose(0,1)\n",
    "print(input_batch1)\n",
    "print(input_batch1.shape)\n",
    "# 이러면 안되는거 아닌가? \n",
    "# 문장이 3개인데 2개로 분류되면 3개의 문장이 2개로 섞이는데...\n",
    "\n",
    "\n",
    "# 잘못생각했네 rnn의 특성상 sequence한 다음 데이터를 예측하기 위해서 앞단어와 다음 단어를 분리한거네 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(input_size = n_class, hidden_size = n_hidden)\n",
    "W = nn.Linear(n_hidden, n_class, bias = False)\n",
    "b = nn.Parameter(torch.ones([n_class]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs,hidden = rnn(input_batch1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2912, -0.5669,  0.1427,  0.3485,  0.4932],\n",
      "         [ 0.2912, -0.5669,  0.1427,  0.3485,  0.4932],\n",
      "         [ 0.2912, -0.5669,  0.1427,  0.3485,  0.4932]],\n",
      "\n",
      "        [[ 0.1172, -0.6765,  0.0636,  0.2234,  0.7838],\n",
      "         [ 0.2039, -0.6064, -0.1160,  0.2507,  0.5864],\n",
      "         [ 0.4487, -0.6771,  0.5561, -0.2510,  0.3231]]],\n",
      "       grad_fn=<StackBackward>)\n",
      "torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "print(outputs)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1172, -0.6765,  0.0636,  0.2234,  0.7838],\n",
      "        [ 0.2039, -0.6064, -0.1160,  0.2507,  0.5864],\n",
      "        [ 0.4487, -0.6771,  0.5561, -0.2510,  0.3231]],\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "outputs1 = outputs[-1]\n",
    "print(outputs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.3234, 1.1556, 0.4484, 1.0180, 0.8028, 0.9104, 1.0893],\n",
      "        [1.3022, 1.1605, 0.6169, 1.0219, 0.9380, 0.8566, 0.9660],\n",
      "        [1.0822, 0.9184, 0.5364, 0.8814, 1.0252, 1.3371, 1.1767]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = W(outputs1) + b\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7906, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(model, target_batch)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
